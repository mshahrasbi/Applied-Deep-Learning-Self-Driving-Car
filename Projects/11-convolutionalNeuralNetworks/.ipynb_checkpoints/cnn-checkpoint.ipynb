{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBoQIkxFYLiW"
   },
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import keras\n",
    "    from keras.datasets import mnist\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Flatten\n",
    "    from keras.layers.convolutional import Conv2D\n",
    "    from keras.layers.convolutional import MaxPooling2D\n",
    "    from keras.layers import Dropout\n",
    "    import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyilGLcaYLis"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MChb4oBrYLi1"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test)= mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8J8QF-1uYLi9",
    "outputId": "4572a846-15ef-4549-f856-f2b7be3f6216"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFMS3dU6YLjG"
   },
   "outputs": [],
   "source": [
    "assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "assert(X_train.shape[1:] == (28,28)), \"The dimensions of the images are not 28 x 28.\"\n",
    "assert(X_test.shape[0] == y_test.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "assert(X_test.shape[1:] == (28,28)), \"The dimensions of the images are not 28 x 28.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "id": "J9y7xRpHYLjP",
    "outputId": "89676408-9395-4b05-b658-97fb50def6d9"
   },
   "outputs": [],
   "source": [
    "num_of_samples=[]\n",
    "cols = 5\n",
    "num_classes = 10\n",
    "\n",
    "fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5,10))\n",
    "fig.tight_layout()\n",
    "     \n",
    "for i in range(cols):\n",
    "    for j in range(num_classes):\n",
    "        x_selected = X_train[y_train == j]\n",
    "        axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))\n",
    "        axs[j][i].axis(\"off\")\n",
    "        if i == 2:\n",
    "            axs[j][i].set_title(str(j))\n",
    "            num_of_samples.append(len(x_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "G5NSj2XmYLjY",
    "outputId": "063b5224-8be4-46e6-8454-55908976ead3"
   },
   "outputs": [],
   "source": [
    "print(num_of_samples)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(0, num_classes), num_of_samples)\n",
    "plt.title(\"Distribution of the train dataset\")\n",
    "plt.xlabel(\"Class number\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYxdAijTYLjg"
   },
   "outputs": [],
   "source": [
    "# previously we would flatten all the images into simple 784 pixels {(1D) -> now the pixels can be fed into a feedforward neural network as inputs}\n",
    "# except now we are leaving it a a 28X28 image but also adding a depth of 1. With regular nueral networks the image had to be flattened into a 1D array\n",
    "# of pixel intensities which were then processed as inputs into the NN while with convolutional networks it is a little different. The first step in preparing our# \n",
    "# data for use in convolutional neural network is to add depth to our data. As mentioned earlier the way CNN works is by applying a filter to the channels of the \n",
    "# image that are being viewed. In the case o grayscale images there is one channel present, therefore our data must reflect the presence of this depth, by adding\n",
    "# this depth of one over data will be in the disired shape to be used as an input for the convolutional layer which we will code with momentarily.\n",
    "\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yd-_rzI6YLjp"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObaaSb2wYLjw"
   },
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuvpUq_TYLj9"
   },
   "outputs": [],
   "source": [
    "# As with any other NN model we must first design it before we are able to use it when it comes to CNN, There are many pre-built architecture that have\n",
    "# already been designed. These pre-built design have been shown to perform effectively and efficiently and gradually being improved over time.\n",
    "# the very first revolutionary CNN was designed back in 1998, this architecture is referred to as the LeNet model and it was designed to be able to\n",
    "# recognize hand written numbrs on checks. After the initial model was described many other CNN architectures have been researched and examined, such as\n",
    "# AlexNet, ZFNet, GoogleNet. Tese newer architectures are more sophisticated, however the initial impact of LeNet should not be  understated as this \n",
    "# architecture type is still used today.\n",
    "# the LeNet architecture typically consists of the following structure: \n",
    "# The first layer of the network is then a Convolutional layer with an unspecified amount of filters, the output of this convolutional layer is then\n",
    "# fed into a pooling layer, that layer is then connected to the another convolutional layer, and finally we have one more pooling layer before I forward \n",
    "# data, is fed into a fully connected layer. That eventually connects to an output classifier.\n",
    "\n",
    "\n",
    "#define the LeNet_model function\n",
    "\n",
    "def LeNet_model():\n",
    "    # we define our model as being Sequential model\n",
    "  model = Sequential()\n",
    "    # we start by definning our very first COnvolutional layer as you have done previously, we use the model.add() function, while the next step\n",
    "    # was to put a Dense layer into our model, This time we are going to start by using a Conv2D layer instead. This convolutional layers consist of\n",
    "    # filters that recognize various features within the image appropriately enough the first arg in Conv2D is going to deal with how many filters \n",
    "    # our layer is going to have, this is a parameter that you can feel free to play around with as a different number of filters will be effective\n",
    "    # for different applications, to our experience we find that 30 filter usually pretty effective.keep in mind that as the number of filters \n",
    "    # increases the number of parameters also increases which demands more computing power.\n",
    "    # next we want to use a tuple to define the size of our filters, this is hypr parameter that you can adjust to get optimal performanace. wer are\n",
    "    # going to use a filter that is 5X5 which is good relative size for our images which are 28X28. As this is the very first layer of our network\n",
    "    # we need to specify the shape of our input data. Recall earlier that we reshaped and added depth to our data to have a shape of 28X28X1.\n",
    "    # so what we will do as we will declare an input shape  is equal to (28, 28, 1), meaning the input layer will take in 28X28 images with depth\n",
    "    # of one with a single channel, that was being fed into the network continuously.\n",
    "    # And finally that last arg for this convolutional layer is the activation function, we are going to use the relu function.\n",
    "    # After this layer our 28X28 image which ever one is being pass through is going to be reduced down to 30 feature maps each one 24X24. Therefore \n",
    "    # the full output volumne of the convolutional layer will have feature maps of up to 30 as each independent filter used on the original image adds\n",
    "    # depth to the convolute image. Analyzing the structure of this layer we can determine the amount of adjustable parameters that it contains. \n",
    "    # Considering that each filter is 5X5 and each pixel inside the filter has its own value, than a single filter will have 25 values or adjustable \n",
    "    # parameters. Considering that we have 30 filters this puts it up to 750. Finally considering that each filter has its own respective bias value\n",
    "    # which is considered a parameter, we get a total value of 780 adjustable (750 + 30 bias = 780 value) parameters in this layer.\n",
    "    # This conv2D function an take some other args to customize the layer. Arguments are Strides and padding.\n",
    "    # Strides, it is simply the kernel of step one involved on the image, how much the kernel is translated. why it is important? well the smaller \n",
    "    # the stride the more the kernel convolution operations and thus the more output that is retained. Ex, 3X3 kernel increments of 2, resulting in the\n",
    "    # following feature map 2X2 as opposed to convolving it by increments of 1 (smaller stride retains more information as more convolutional operations\n",
    "    # are conducted)\n",
    "    # padding, when conducting convolutional operations the spatial size of the image gets smaller as we lose the information at the borders, even if we \n",
    "    # use a stride of one, so padding works to perserve the spatail dimensionality of the image. Let's look at half padding (same padding) it ensures\n",
    "    # that the output size remains the same as the input size. Suppose we have a 5X5 image with the corresponding 4X4 kernel to ensure that the output is\n",
    "    # the same as the inputs we add 2 pixels thick layers of padding each pixel with a value of zero, and convolving the kernel, notice the information at \n",
    "    # the borders is now processed thus outputing an image with the same dimensionality preserving spatial dimensionality in most cases is quit a  desirable\n",
    "    # property. for instance it allows to extract low level features and thus by keeping all the information at the border the tends to improve performance.\n",
    "    # now the padding argument can take 3 variations either 'valid' or 'casual' or 'same' and default is no padding will be used. \n",
    "  model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    # The next layer we add to our model is going to be the pooling layer, it simplely adding MaxPooling2D layer. The function only takes in one argument \n",
    "    # which is of the size of the pooling element.We use a tuple to define this pooling size, the pool_size that we are going to use is going to be a 2X2\n",
    "    # Therefore grabbing the maximum value in a feature map within a 2X2 neighborhood. The pooling size is appropraite as it is not too large as to miss\n",
    "    # relevant features in the convoluted image. Howeve it is large enough to help classify our image in a more efficient way. After pooling process the \n",
    "    # shape of our convoluted image is going to go from a 24X24X30 to a smaller shape of a 12X12X30 which should make sense to you, since max pooling \n",
    "    # scales down every feature map into a smaller obstructured representation and since we are using 2X2 neighborhood it makes sense that the image size \n",
    "    # was reduced to half of what it was previously.\n",
    "    # overall as we move forward into the network the size of our image decreases and start the gain depth in our specific case gaining a depth of 30. The \n",
    "    # depth contains features extracted from the original image from each of the filters and it very valuable. \n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # We are going to add another conv2D layer, since we are dealing with a more dense input for this layer we are going to use fewer filters for for the \n",
    "    # purpose of minimizing the required computational power. we are going to use 15 filters on this layer as our image is smaller this time around we are \n",
    "    # going to use smallar filter to extract features whether our filter size is going to be a 3X3, ad interestingly enough it is important to note that \n",
    "    # even though we were using fewer filter in this layer and we are using a small filter size we are going to be dealing with a higher number of \n",
    "    # parameters. This is due to the fact that the output that is coming out of the pooling layer and going into the nextconvolutional layer as an input\n",
    "    # now has a depth of 30. This means that each filter must be applied to each depth a layer which results in 15X30X3X3 = 4050 parameters.We also add a\n",
    "    # biased parameter for each of the filters that we are using which results in a total of 4050 + 15 = 4065 parameters.\n",
    "    # this is a little over 5 times as many parameters as our initial input convolutional layer. So you can see that this convultion process begins to\n",
    "    # demand more and more computational power, as the input images start to increase in depth along the network. And as this is not first layer in the \n",
    "    # network there is no need to define the input_shape we already previous steps.\n",
    "  model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    #\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Next layer is Flatten, does not actually require any paramters. However it will help us flatten our data in order to format it properly for it to \n",
    "    # go in the fully connected network which makes sense because if you are a member in the deep learning code that we implemented earlier before processing \n",
    "    # our image into the neural network we had to flatten it.we had to flatten each image to be 1D. Flattened layer will take our output data with a shape of \n",
    "    # 5X5X15 and reshape it into a 1D array of shape 375. Now our data is ready to be fed into a fully connected layer. \n",
    "  model.add(Flatten())\n",
    "    # Now add dense layer, we first defined the amount of nodes we want to add in the first hidden layer, we will actully add 500 nodes to this layer.\n",
    "    # A lower number will typically provide minimally less accuracy over a higher number will require more computing power. We also define the \n",
    "    # activation function to be the relu function \n",
    "  model.add(Dense(500, activation='relu'))\n",
    "    #\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "    #\n",
    "  model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "btiEreyaeazC",
    "outputId": "cbd30afb-a02e-4f89-dcfe-e1134bd8222b"
   },
   "outputs": [],
   "source": [
    "model = LeNet_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "HnhpqIHPmcbp",
    "outputId": "c88bc8c3-2ae0-4473-ff2e-515fc9564b0e"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=400, verbose=1, shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "1cLwpLApnzpI",
    "outputId": "2f606637-5a87-40af-b165-306b18473c0a"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "9VF_0Un4oOVf",
    "outputId": "68db922e-e018-4fc7-c8f3-762347084134"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "EMkYtmC5ovto",
    "outputId": "dbbdbfb0-dbf9-4a3c-82a5-b182f222c8c6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "     \n",
    "url = 'https://www.researchgate.net/profile/Jose_Sempere/publication/221258631/figure/fig1/AS:305526891139075@1449854695342/Handwritten-digit-2.png'\n",
    "response = requests.get(url, stream=True)\n",
    "img = Image.open(response.raw)\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "czQpujE2p4JM",
    "outputId": "1e77f1b1-4b45-48bc-b8f0-7973afd786eb"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "     \n",
    "img = np.asarray(img)\n",
    "img = cv2.resize(img, (28, 28))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.bitwise_not(img)\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RByuLGxIqCfs"
   },
   "outputs": [],
   "source": [
    "img = img/255\n",
    "img = img.reshape(1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y0otMSyHqNEv",
    "outputId": "ac9f27e3-a9ee-4bac-88d8-b27f40cfbb96"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(img)\n",
    "print(\"predicted digit:\", str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(type(score))\n",
    "print('Test score: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally whenever the validation error is higher than the training error often times that would mark the beginning of your model\n",
    "# starting to overfit. So as affective as this Convolutional layer was in classifying test images on top of that we are going to \n",
    "# introduce another technique to reduce overfitting of our data.we are going to go back to our LeNet model function and we are gooing\n",
    "# to add a specific layer type that will help reduce overfitting of our data. This layer type is called the 'dropout' layer.\n",
    "# This layer essentially functions by randomly setting a fraction our rate of input units to zero at each update during training which \n",
    "# helps prevenet overfitting. Some nodes will be turn off and are no longer communicating information along the network. As mentioned\n",
    "# earlier the dropout function randomly select a predefined fraction of nodes to be turned off.What this means is that every time the \n",
    "# network updates parameters during the training process, it selects random nodes that it is going to turn off. So while some nodes \n",
    "# that have been turned off it is important to note that a different set of nodes are turned off with each update, this process works to\n",
    "# prevent overfitting because it forces the neural network to use various combinations of nodes to classify the same data as with each \n",
    "# updates a random set of nodes survives the dropout process. Each node is forced to learn the data set in a more independent way with a\n",
    "# unique neural arrangement each time. This causes the NN in general to become more versatile and allows the weights to be more uniformly \n",
    "# distributed along that network.\n",
    "# Now this only occurs during training when this network is used on new data, test data for example, dropout is not used in all the nerual\n",
    "# nodes are used to classify the new data. One dropout is no longer used, the nodes can combined all of their independent learning to form\n",
    "# a more effective NN. This results in a network that has reduced generalized error and demonstrates less overfitting when it is exposed \n",
    "# to new data.  \n",
    "# Example of this to imagine the following case:\n",
    "# Imagine a simple perceptron model with 2 inputs knowns and one output node, Our system is relatively simple and it functions by having one\n",
    "# neuron and two feed data into neuron 3. Neuron 3 combines this data to give an output. Now let's assume our network functions as follows; \n",
    "# neuron one has been well trained and feeds the correct input to neron 3 about 90% of the time, while neron 2 doesn't perform as well and \n",
    "# seems to provide accurate input to neuron 3 at random. Therefore as the network receives more training it simply kearns to listen fully\n",
    "# to neuron one and completely ignores the input associated with neuron number 2. We essentially get a perceptron that behaves as follow:\n",
    "# --> 1 --> 3 --> output\n",
    "# This can be viewed as redundant because two neurons are essentially just behaving as one neuron 2 becomes obsolete. So this is were \n",
    "# dropout plays a key role in improving the effectiveness of the network. Applying dropout to this network means that at random instances \n",
    "# either a neuron 1 or neuron 2 has turned off. when neuron 2 is turned off, not much changes as the network was ignoring this neuron anyways.\n",
    "# However in the cases when neuron 1 is turned off the NN is forced to listen to neuron 2, while at the beginning this might seem like be a bad\n",
    "# situation due to the poor performance of neuron 2. It is actually a good thing. This is because these situations force the 2nd neuron to\n",
    "# learn and adopt to the data as well, independently of neuron 1. This also forces the NN to listen to neuron 2 and provides resources\n",
    "# towards improving the accuracy of neuron 2. Now when the network moves on that testing data neurons 1 and 2, start to work togather more \n",
    "# efficiently, while neuron one is still correct 90% of the time, neuron 2 has also been trained and provides and added level of accuracy\n",
    "# that pushes the accuracy of our network over 90%. This is just a simple example of how to wrap up layers can provide uniform weight\n",
    "# distribution over the network and can decresse generalization there. In more complex networks this results in decreased overfitting.\n",
    "\n",
    "# Now we import 'Dropout' and we update LeNet model and replace the dropout layer within our NN for the purpose of demonstration, we are\n",
    "# only going to use a single dropout layer. However more than one dropout layer can be used in a given network to obtain the desired \n",
    "# performance. The location of these layers in a network can also vary, you can place them between the convolutional layers or between \n",
    "# your fully connected layers. Typically the dropout layer is used in between layers that have a high number of parameters, because\n",
    "# these high parameter layers are more likely to overfit and memorized the training data. And for this reason we will attempt to place \n",
    "# our dropout layer between our two fully connected ones.To fix our overfitting problem. \n",
    "\n",
    "def LeNet_model_1():\n",
    "    # we define our model as being Sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    # Dropout layer takes an argument for a fraction rate, there is refers to the amount of input nodes that the dropout layer\n",
    "    # drops during each update with zero refer to when no nodes are dropped and one referring to all input nodes are dropped.\n",
    "    # we will use 0.5 as this is the recomended rates provided by the researchers that first proposed the dropout technique \n",
    "    # to reduce overfitting\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LeNet_model_1()\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model_1.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=400, verbose=1, shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see our validation accuracy jumps up to match our training accuracy. This an also be seen in the error plot and shows that our\n",
    "# overfitting issue was successfully taken care of and just notice how the validation error remains lower than your training error which can also\n",
    "# signify a reduction in overfitting as the NN was generalized to also correctly classify the validation data with minimal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['loss'])\n",
    "plt.plot(history_1.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['acc'])\n",
    "plt.plot(history_1.history['val_acc'])\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "     \n",
    "url = 'https://www.researchgate.net/profile/Jose_Sempere/publication/221258631/figure/fig1/AS:305526891139075@1449854695342/Handwritten-digit-2.png'\n",
    "response = requests.get(url, stream=True)\n",
    "img = Image.open(response.raw)\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "     \n",
    "img = np.asarray(img)\n",
    "img = cv2.resize(img, (28, 28))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.bitwise_not(img)\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img/255\n",
    "img = img.reshape(1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_1.predict_classes(img)\n",
    "print(\"predicted digit:\", str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_1.evaluate(X_test, y_test, verbose=0)\n",
    "print(type(score))\n",
    "print('Test score: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is evident that the CNN architecture helped us improve our training and validation and accuracy while decreasing the amount of \n",
    "# overfitting persent in our NN. We then event futher decreased the degree of overfitting by adding a dropout layer to the network\n",
    "# and now that we have optimized our CNN we believe we are now able to use it for more complex image classification problems\n",
    "\n",
    "# However before we are ready to move on to more complex datasets let's ensure that we have a concrete grasp of just how CNN function,\n",
    "# As mentioned earlier these convolutional layers take an input image and use a smaller filter to extract features from the image\n",
    "# more than one filter can be applied within each layer and the size of the filters can vary as well.\n",
    "# In our code, our first layer contains 30 filters of size 5X5 while our second layer contains 15 folders of 3X3. The purpose of using \n",
    "# multiple filters per layer is that each filter learns to look for its unique set of features and can analyze the image in a unque way.\n",
    "# the analysis of all the filters can then be combined to get a network with higher degree of accuracy \n",
    "\n",
    "# Let's attempt to actually visualize what each of our filters are seeing into code.This will help us see the variety of features of the \n",
    "# filters that tact and how they can work together to accurately classify data sets.\n",
    "# We will begin by instantiating the two layers that we want to see the output for, this process basically requires us to export layers \n",
    "# from our fully defined model se we can log the parts of the model at various sections along the network, we can do so by using something\n",
    "# called the Model class API. Model class API is an incredibly useful tool as it allows us to define a model much in the same way as the\n",
    "# Sequential function we have become familiar with, using the Model class API allows us to instantiate layers from pre-trained models\n",
    "# effectively allowing us to reuse sections of previously trained models. We aregoing to take advantage of this ability to help us visualize\n",
    "# the outputs from out two convolutional layers. \n",
    "# Model function requires 2 arguments, the first arguement defines all the inputs into your network, while he 2nd argument defines it outputs \n",
    "# that you want from the model. The case of layer1 our inputs are defined as the inputs into our very first layer. Therefore we use a model that layers\n",
    "# to access the layers within our model and we use the index position of 0 to access our first layer in the model, to access the first convolutional\n",
    "# layer if you view the model summary we printed, you can recognize that our first convolutional layer is the first layer in our network. and so \n",
    "# accessing the inputs to this layer is done by adding the 'input' extension to our layer. We then define the outputs of the same layer using\n",
    "# 'model.layers[0].outputs' once again we access the first index position and this time we acess the outputs of this layer. Our first layer is complete.\n",
    "layer1 = Model(inputs=model.layers[0].input, outputs=model.layers[0].output)\n",
    "layer2 = Model(inputs=model.layers[0].input, outputs=model.layers[2].output)\n",
    "\n",
    "# Now that we finished creating our two layers let's run a prediction on them without importing image. you get this prediction by calling predict fucntion\n",
    "visual_layer1, visual_layer2 = layer1.predict(img), layer2.predict(img)\n",
    "print(visual_layer1.shape)\n",
    "print(visual_layer2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(30):\n",
    "    plt.subplot(6, 5, i+1)\n",
    "    plt.imshow(visual_layer1[0, :, :, i], cmap=plt.get_cmap('jet'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output of our 2nd layer in much the same way as earlier, red signifying high intensity while the blue signifies low intensity.\n",
    "# These images appear to be a bit more abstarct and this makes perfect sense, we always talked about how the feature maps starts to\n",
    "# become unrecognizable towards the very end as they contain less information about the image, but more information about the specific\n",
    "# feature that is distinct to the kernel that was involved on the image, since the deeper you get into the NN, the filters become more and \n",
    "# more complex building on top of one another and becoming more sophisticated in terms of the high of shapes that they start in coding for\n",
    "# these high level shapes as present in very specific parts of the image ans so which end up with our filters that only retain the information\n",
    "# in the image taht's relevant to it \n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(15):\n",
    "    plt.subplot(3, 5, i+1)\n",
    "    plt.imshow(visual_layer2[0, :, :, i], cmap=plt.get_cmap('jet'))\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
